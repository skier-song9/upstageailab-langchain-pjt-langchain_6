from importlib import metadata
from typing import List, Dict, Any, Optional
import re
from collections import defaultdict
from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_openai import ChatOpenAI

def format_context(context: List[Document]) -> str:
    """
    LangChain Document ë¦¬ìŠ¤íŠ¸ë¥¼ LLM í”„ë¡¬í”„íŠ¸ì— ì í•©í•œ ë‹¨ì¼ ë¬¸ìì—´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    ê° ë…¼ë¬¸ì˜ ì¶œì²˜(ë¡œì»¬ DB, Tavily, OpenAlex)ë¥¼ í•¨ê»˜ í‘œì‹œí•©ë‹ˆë‹¤.
    
    Args:
        context: 'title'ì„ metadataì— í¬í•¨í•˜ëŠ” Document ê°ì²´ ë¦¬ìŠ¤íŠ¸
        
    Returns:
        ê° ë…¼ë¬¸ ì •ë³´ì™€ ì¶œì²˜ê°€ í¬í•¨ëœ ì „ì²´ ë¬¸ìì—´
    """
    context_parts = []
    for i, doc in enumerate(context):
        # doc.metadataì—ì„œ 'title'ì„, doc.page_contentì—ì„œ ì´ˆë¡ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
        title = doc.metadata.get('title', 'No Title Provided')
        abstract = doc.page_content
        
        # ì¶œì²˜ íŒë³„í•˜ê¸°
        source = determine_paper_source(abstract)
        
        context_parts.append(f"--- Paper {i+1} ({source}) ---\nTitle: {title}\nAbstract: {abstract}")
    
    return "\n\n".join(context_parts)

def determine_paper_source(content: str) -> str:
    """
    ë…¼ë¬¸ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ ì–´ëŠ ì†ŒìŠ¤ì—ì„œ ê°€ì ¸ì™”ëŠ”ì§€ íŒë³„í•©ë‹ˆë‹¤.
    
    :param content: ë…¼ë¬¸ì˜ ë‚´ìš© (page_content)
    :return: ì¶œì²˜ ë¬¸ìì—´
    """
    # ë¡œì»¬ ë°ì´í„°ë² ì´ìŠ¤ ì‹ë³„ íŒ¨í„´
    if "ğŸ“„" in content and "ìœ ì‚¬ë„:" in content and "ì¸ìš©ìˆ˜:" in content:
        return "ë¡œì»¬ ë²¡í„° DB"
    
    # OpenAlex ì‹ë³„ íŒ¨í„´
    elif "ğŸ“š" in content and "ì €ì:" in content and ("ğŸ“Š ì¸ìš©ìˆ˜:" in content or "ğŸ“ ìš”ì•½:" in content):
        return "OpenAlex í•™ìˆ  ê²€ìƒ‰"
    
    # Tavily ì‹ë³„ íŒ¨í„´
    elif "ğŸ¤– AI ìš”ì•½:" in content or "ğŸ”—" in content or "ğŸŒ" in content:
        return "Tavily ì›¹ ê²€ìƒ‰"
    
    # êµ¬ì²´ì ì¸ íŒ¨í„´ìœ¼ë¡œ ì¬ì‹œë„
    elif "**ë¡œì»¬ ë°ì´í„°ë² ì´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼:**" in content:
        return "ë¡œì»¬ ë²¡í„° DB"
    elif "**OpenAlex í•™ìˆ  ê²€ìƒ‰ ê²°ê³¼:**" in content:
        return "OpenAlex í•™ìˆ  ê²€ìƒ‰" 
    elif "**ì›¹ ê²€ìƒ‰ ê²°ê³¼ (ìµœì‹  ì •ë³´):**" in content:
        return "Tavily ì›¹ ê²€ìƒ‰"
    
    # ê¸°ë³¸ê°’
    else:
        return "ë¯¸ë¶„ë¥˜"

def llm_generate(question: str, context: List[Document], llm_api_key: str) -> str:
    """
    ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•˜ëŠ” LLM í•¨ìˆ˜.
    ë…¼ë¬¸ë“¤ì„ ë¶„ì„í•˜ì—¬ êµ¬ì¡°í™”ëœ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
    1. Documentì—ëŠ” title(ë…¼ë¬¸ ì œëª©)ê³¼ content(ë…¼ë¬¸ abstract ë‚´ìš©)ê°€ ìˆë‹¤.
    2. contextì˜ ê¸¸ì´ê°€ 0ì´ë©´ "ê²€ìƒ‰ëœ í›„ì†ë…¼ë¬¸ì´ ì—†ë‹¤"ëŠ” ë‚´ìš©ì˜ ë‹µë³€ì„ ë°˜í™˜í•˜ì—¬ë¼. (LLM ì‚¬ìš© ê¸ˆì§€)
    3. format_context í•¨ìˆ˜ë¥¼ í†µí•´ contextì˜ ê° Documentë“¤ì„ `title: ë…¼ë¬¸ ì œëª©, abstract: ë…¼ë¬¸ abstract ë‚´ìš©` ìœ¼ë¡œ ì¬êµ¬ì„±í•˜ì—¬ context_str ì— ì €ì¥í•˜ì—¬ë¼.
    4. promptë¥¼ ì‚¬ìš©í•˜ì—¬ LangChain ë¬¸ë²•ì— ë”°ë¼ LLM(openai GPT-3.5 Turbo)ìœ¼ë¡œë¶€í„° ë‹µë³€ì„ ìƒì„±í•˜ì—¬ë¼.
    
    :param str question: ì‚¬ìš©ìê°€ ì…ë ¥í•œ í”„ë¡¬í”„íŠ¸
    :param List[Document] context: ê²€ìƒ‰ëœ í›„ì† ì—°êµ¬ ë…¼ë¬¸ë“¤ì˜ ë¦¬ìŠ¤íŠ¸
    :param llm_api_key: OpenAI API í‚¤
    :return str: êµ¬ì¡°í™”ëœ ë‹µë³€ ë¬¸ìì—´
    """
    print("ğŸ¤– LLM ë‹µë³€ ìƒì„± ì¤‘...")
    print(f"ğŸ” DEBUG: ë°›ì€ ì§ˆë¬¸ = '{question}'")
    print(f"ğŸ“„ DEBUG: ì»¨í…ìŠ¤íŠ¸ ê°œìˆ˜ = {len(context)}")
    
    # 2. context ë¦¬ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
    if not context:
        print("â„¹ï¸ ì»¨í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆì–´ LLMì„ í˜¸ì¶œí•˜ì§€ ì•Šê³  ê¸°ë³¸ ë©”ì‹œì§€ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.")
        return "ê²€ìƒ‰ëœ í›„ì† ë…¼ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰í•´ ë³´ì„¸ìš”."
    
    # 3. contextë¥¼ í”„ë¡¬í”„íŠ¸ì— ë„£ê¸° ì¢‹ì€ ë‹¨ì¼ ë¬¸ìì—´ë¡œ formattingí•œë‹¤.
    context_str = format_context(context)

    # 4. LangChain í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì„ ì •ì˜í•©ë‹ˆë‹¤.
    prompt_template = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                """You are a world-class AI research assistant. Your primary goal is to provide a clear, structured, and insightful analysis of academic papers based on their titles and abstracts.
You must synthesize information from multiple sources and present it in an easy-to-digest format for researchers.
Your response must be well-organized, using Markdown for headings and lists.
"""
            ),
            (
                "human",
                """ë‹¤ìŒ ì—°êµ¬ ë…¼ë¬¸ë“¤ì„ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µí•´ì£¼ì„¸ìš”.

**ì‚¬ìš©ì ì§ˆë¬¸:**
<question>
{question}
</question>

**ì œê³µëœ ë§¥ë½ (í›„ì† ì—°êµ¬ ë…¼ë¬¸ë“¤):**
<context>
{context_str}
</context>

**ë‹µë³€ ì§€ì¹¨:**
1. **ì§ˆë¬¸ ìœ í˜•ì„ ë‚´ë¶€ì ìœ¼ë¡œ íŒë‹¨í•˜ë˜, íŒë‹¨ ê³¼ì •ì„ ë‹µë³€ì— í‘œì‹œí•˜ì§€ ë§ˆì„¸ìš”.**

2. **ì—°êµ¬ ê´€ë ¨ ì§ˆë¬¸ì¸ ê²½ìš°:**
   - **ë‹¤ì–‘í•œ ì¶œì²˜ì—ì„œ ê· í˜•ìˆê²Œ ì •ë³´ë¥¼ ì„ íƒ**í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.
   - **ë¡œì»¬ DB, OpenAlex, Tavily ê°ê°ì—ì„œ ìµœì†Œ 1ê°œì”©**ì€ í¬í•¨í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.
   - ê° ë…¼ë¬¸ì— ëŒ€í•´ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ 1-2ì¤„ ìš”ì•½ì„ ì œê³µí•˜ê³ , **ë°˜ë“œì‹œ ì¶œì²˜ë¥¼ í‘œì‹œí•˜ì„¸ìš”**.
   - ì¶œì²˜ í‘œì‹œ ë°©ë²•:
     - ë¡œì»¬ ë²¡í„° DB â†’ ğŸ“š (ë¡œì»¬ DB)
     - OpenAlex í•™ìˆ  ê²€ìƒ‰ â†’ ğŸ“ (OpenAlex)
     - Tavily ì›¹ ê²€ìƒ‰ â†’ ğŸŒ (Tavily)
   - ì§ˆë¬¸ì— ë”°ë¼ ë‹µë³€ ìŠ¤íƒ€ì¼ì„ ì¡°ì •í•˜ì„¸ìš”:
     - "ì£¼ìš” í›„ì† ì—°êµ¬" â†’ ì—°êµ¬ ëª©ë¡ê³¼ ë¶„ì•¼ë³„ ë¶„ë¥˜
     - "ê¸°ìˆ ì  ì˜í–¥" â†’ êµ¬ì²´ì ì¸ ê¸°ìˆ ì  ê¸°ì—¬ë„ ë¶„ì„
     - "ë‹¨ì  ë³´ì™„" â†’ ë¬¸ì œì ê³¼ í•´ê²°ì±… ì¤‘ì‹¬
     - "í•µì‹¬ í˜ì‹ " â†’ í˜ì‹ ì  ìš”ì†Œë“¤ ê°•ì¡°
     - "ì˜í–¥" â†’ íŒŒê¸‰ íš¨ê³¼ì™€ ë³€í™” ì¤‘ì‹¬
   - **ì¶œì²˜ë³„ë¡œ ë‹¤ì–‘ì„±ì„ í™•ë³´í•˜ì—¬** í¬ê´„ì ì¸ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.

3. **ì¼ë°˜ì ì¸ ì§ˆë¬¸ì¸ ê²½ìš°:**
   - ë…¼ë¬¸ ë‚´ìš©ì„ ì–¸ê¸‰í•˜ì§€ ë§ˆì„¸ìš”.
   - ë…¼ë¬¸ ì¶”ì²œì´ë‚˜ ìš”ì•½ì„ í•˜ì§€ ë§ˆì„¸ìš”.
   - ê°„ë‹¨í•˜ê³  ì ì ˆí•œ ë‹µë³€ë§Œ ì œê³µí•˜ì„¸ìš”.

**ì¤‘ìš”ì‚¬í•­:**
- ë‹µë³€ì— "ë‹µë³€:", "ì§ˆë¬¸ íŒë‹¨:" ë“±ì˜ ë ˆì´ë¸”ì„ ë¶™ì´ì§€ ë§ˆì„¸ìš”.
- ë‚´ë¶€ íŒë‹¨ ê³¼ì •ì„ ì‚¬ìš©ìì—ê²Œ ë³´ì—¬ì£¼ì§€ ë§ˆì„¸ìš”.
- ë°”ë¡œ ìµœì¢… ë‹µë³€ë§Œ ì œê³µí•˜ì„¸ìš”.
- ëª¨ë“  ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”.
- ë‹µë³€ì€ ëª…í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.
- ê°™ì€ ë‚´ìš©ì„ ë°˜ë³µí•˜ì§€ ë§ˆì„¸ìš”.
- ë‹µë³€ì´ ì™„ë£Œë˜ë©´ ìì—°ìŠ¤ëŸ½ê²Œ ì¢…ë£Œí•˜ì„¸ìš”.
"""
            ),
        ]
    )

    # LLM ëª¨ë¸ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤. (GPT-4o-mini ì‚¬ìš©)
    llm = ChatOpenAI(
        model_name="gpt-4o-mini", 
        api_key=llm_api_key, 
        temperature=0.2,  # ì¼ê´€ì„± ìˆëŠ” ë‹µë³€
        max_tokens=1000,  # ìµœëŒ€ í† í° ì œí•œ
        frequency_penalty=0.5,  # ë°˜ë³µ íŒ¨ë„í‹°
        presence_penalty=0.3    # ì¡´ì¬ íŒ¨ë„í‹°
    )
    
    # LangChain Expression Language (LCEL)ì„ ì‚¬ìš©í•˜ì—¬ ì²´ì¸ì„ êµ¬ì„±í•©ë‹ˆë‹¤.
    # 1. í”„ë¡¬í”„íŠ¸ í¬ë§·íŒ… -> 2. LLM í˜¸ì¶œ -> 3. ì¶œë ¥ íŒŒì‹±(ë¬¸ìì—´ë¡œ)
    chain = prompt_template | llm | StrOutputParser()
    
    # ì²´ì¸ì„ ì‹¤í–‰í•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
    answer = chain.invoke({
        "question": question,
        "context_str": context_str
    })
    
    return answer