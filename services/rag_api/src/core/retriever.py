from typing import List
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.pydantic_v1 import BaseModel, Field
from langchain_core.output_parsers import StrOutputParser, JsonOutputParser
from langchain_upstage import ChatUpstage
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_tavily import TavilySearch
from langchain_core.tools import tool

import os 
from dotenv import load_dotenv
ROOT_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..', '..', '..')) # 5ë‹¨ê³„ ìœ„ë¡œ ì´ë™
load_dotenv(os.path.join(ROOT_DIR, '.env'))

def mock_rag_retrieval(paper_title: str) -> List[str]:
    """Vector Storeì—ì„œ ê´€ë ¨ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ëŠ” RAG Retriever ëª¨ì˜ í•¨ìˆ˜."""
    print(f"ğŸ” Vector Store ê²€ìƒ‰ (Retrieve): '{paper_title}' ê¸°ë°˜ í›„ì† ë…¼ë¬¸")
    return ["í›„ì† ë…¼ë¬¸ A (from Vector Store)", "í›„ì† ë…¼ë¬¸ B (from Vector Store)"]

@tool
def get_korean_definition(keyword: str) -> str:
    """ì£¼ì–´ì§„ í•œêµ­ì–´ í‚¤ì›Œë“œ(keyword)ì— ëŒ€í•œ ê°„ê²°í•œ í•œ ì¤„ ì •ì˜ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
    
    :param keyword: ì •ì˜ë¥¼ ì°¾ì„ í•œêµ­ì–´ í‚¤ì›Œë“œ
    :return: ê²€ìƒ‰ëœ í‚¤ì›Œë“œì˜ í•œ ì¤„ ì •ì˜
    """
    # Tavily SearchëŠ” í•¨ìˆ˜ê°€ í˜¸ì¶œë  ë•Œë§ˆë‹¤ API í‚¤ì™€ í•¨ê»˜ ì´ˆê¸°í™”ë©ë‹ˆë‹¤.
    # ë˜ëŠ” ì „ì—­ ë³€ìˆ˜ë¡œ í•œ ë²ˆë§Œ ì„ ì–¸í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.
    tavily_api_key = os.getenv("TAVILY_SEARCH")
    if not tavily_api_key:
        return f"Tavily API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        
    search = TavilySearchResults(api_key=tavily_api_key, max_results=1)
    search_query = f'"{keyword}"ì— ëŒ€í•œ í•œ ì¤„ ì •ì˜'
    result = search.invoke(search_query)
    
    if result and 'content' in result[0]:
        return result[0]['content']
    return f"'{keyword}'ì— ëŒ€í•œ ì •ì˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

# 1. í‚¤ì›Œë“œ ì¶”ì¶œì„ ìœ„í•œ ë°ì´í„° êµ¬ì¡° ì •ì˜ (Pydantic ëª¨ë¸)
class Keywords(BaseModel):
    """A list of keywords extracted from the user's question."""
    keywords: List[str] = Field(description="ì‚¬ìš©ì ì§ˆë¬¸ì—ì„œ ì¶”ì¶œëœ í•µì‹¬ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸")


def augment_prompt(question: str, llm_api_key: str, tavily_search_key: str) -> str:
    """ì‚¬ìš©ì promptì—ì„œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ì—¬ tavily searchë¡œ ì¦ê°•í•œ í›„, ì˜ì–´ë¡œ ë²ˆì—­í•˜ì—¬ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜
    1. Upstageì˜ solar-mini LLM ëª¨ë¸ì„ ì‚¬ìš©í•´ questionìœ¼ë¡œë¶€í„° í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ì—¬ë¼. ì´ë•Œ, LLMì˜ ë‹µë³€ì´ List[str] ì´ ë˜ë„ë¡ í˜•ì‹ì„ ì œí•œí•˜ëŠ” í”„ë¡¬í”„íŠ¸ë¥¼ ì˜ ì‘ì„±í•˜ì—¬ë¼. ë˜ëŠ” Langchainì—ì„œ OutputFixingParserì™€ ê°™ì€ í´ë˜ìŠ¤ë¥¼ í™œìš©í•˜ì—¬ ì¶œë ¥ í˜•ì‹ì„ ì œí•œí•˜ì—¬ë¼.

    2. 1ë²ˆì—ì„œ êµ¬í•œ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ì˜ ê° ë‹¨ì–´ë“¤ì„ tavily-search ë¥¼ ì‚¬ìš©í•´ ê° í‚¤ì›Œë“œì— ëŒ€í•´ í•œ ì¤„ì§œë¦¬ ë¶€ê°€ì„¤ëª…ì„ êµ¬í•˜ì—¬ë¼.

    3. questionì˜ í‚¤ì›Œë“œì— 2ë²ˆì—ì„œ êµ¬í•œ ë¶€ê°€ì„¤ëª…ì„ í‚¤ì›Œë“œ ë‹¨ì–´ ë’¤ì— ê´„í˜¸ ì•ˆì— ì¶”ê°€í•˜ì—¬ë¼.
    e.g. ê¸°ì¡´ question : Downstream taskì— ëŒ€í•´ ëª¨ë¸ì˜ ì¬ì‚¬ìš©ì„±ì„ í–¥ìƒì‹œí‚¨ í›„ì†ë…¼ë¬¸ì„ ì•Œë ¤ì¤˜.
        augmented_question : Downstream task(AIì™€ ë¨¸ì‹ ëŸ¬ë‹ì—ì„œ ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì„ íŠ¹ì • ì‘ì—…ì— ë§ê²Œ ì¡°ì •í•˜ì—¬ í™œìš©í•˜ëŠ” ê³¼ì •)ì— ëŒ€í•´ ëª¨ë¸ì˜ ì¬ì‚¬ìš©ì„±(ì´ë¯¸ ê°œë°œëœ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì„ ë‹¤ë¥¸ ë¬¸ì œë‚˜ í™˜ê²½ì— ì ìš©í•˜ì—¬ í™œìš©í•˜ëŠ” ê²ƒì„ ì˜ë¯¸)ì„ í–¥ìƒì‹œí‚¨ í›„ì†ë…¼ë¬¸ì„ ì•Œë ¤ì¤˜.

    4. Upstageì˜ solar-pro2 LLM ëª¨ë¸ì„ ì‚¬ìš©í•´ questionì„ ì˜ì–´ë¡œ ë²ˆì—­í•˜ì—¬ return

    :param str question: user prompt
    :return str: augmented prompt & translated to English
    """
    # 1. solar-mini ë¡œ í‚¤ì›Œë“œ ì¶”ì¶œ
    # llm_mini = ChatUpstage(api_key=llm_api_key, model='solar-mini')
    # # JSON í˜•ì‹ìœ¼ë¡œ ì¶œë ¥ì„ íŒŒì‹±í•˜ëŠ” íŒŒì„œ ì„¤ì •
    # parser = JsonOutputParser(pydantic_object=Keywords)
    # # í‚¤ì›Œë“œ ì¶”ì¶œì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜
    # keyword_prompt = ChatPromptTemplate.from_template(
    #     """You are an expert in extracting keywords from a text.
    #     Extract the main keywords from the following user question.
    #     Your output must be a JSON object with a single key 'keywords' containing a list of the extracted keywords.
    #     Exclude keywords related to 'follow-up papers', 'í›„ì† ë…¼ë¬¸'.

    #     Question: {question}

    #     {format_instructions}"""
    # )
    # # LCELì„ ì‚¬ìš©í•´ í‚¤ì›Œë“œ ì¶”ì¶œ ì²´ì¸ êµ¬ì„±
    # keyword_chain = keyword_prompt | llm_mini | parser
    # # ì²´ì¸ ì‹¤í–‰
    # response = keyword_chain.invoke({
    #     "question": question,
    #     "format_instructions": parser.get_format_instructions()
    # })
    # keywords = response['keywords']
    # print(f"âœ… ì¶”ì¶œëœ í‚¤ì›Œë“œ: {keywords}")

    # # --- 2ë‹¨ê³„: Tavily Searchë¡œ ê° í‚¤ì›Œë“œì— ëŒ€í•œ ë¶€ê°€ì„¤ëª… ê²€ìƒ‰ ---
    # # Tavily Search ë„êµ¬ ì´ˆê¸°í™”
    # search = TavilySearchResults(api_key=tavily_search_key, max_results=1)
    # keyword_definitions = {}

    # print("\n--- 2. í‚¤ì›Œë“œ ì •ì˜ ê²€ìƒ‰ ì¤‘... ---")
    # for keyword in keywords:
    #     # ê° í‚¤ì›Œë“œì— ëŒ€í•œ í•œ ì¤„ ì •ì˜ë¥¼ ì–»ê¸° ìœ„í•´ êµ¬ì²´ì ì¸ ì¿¼ë¦¬ ìƒì„±
    #     search_query = f'"{keyword}"ì— ëŒ€í•œ í•œ ì¤„ ì •ì˜'
    #     search_result = search.invoke(search_query)
        
    #     # ê²€ìƒ‰ ê²°ê³¼ê°€ ìˆê³ , content í‚¤ê°€ ì¡´ì¬í•˜ë©´ ì •ì˜ ì¶”ì¶œ
    #     if search_result and 'content' in search_result[0]:
    #         definition = search_result[0]['content']
    #         keyword_definitions[keyword] = definition
    #         print(f"âœ… {keyword}: {definition}")

    # # --- 3ë‹¨ê³„: ì›ë³¸ ì§ˆë¬¸ì— ë¶€ê°€ì„¤ëª… ì¶”ê°€í•˜ì—¬ ì¦ê°• ---
    
    # augmented_question = question
    # print("\n--- 3. í”„ë¡¬í”„íŠ¸ ì¦ê°• ì¤‘... ---")
    # for keyword, definition in keyword_definitions.items():
    #     # ì›ë³¸ ì§ˆë¬¸ì—ì„œ í‚¤ì›Œë“œë¥¼ 'í‚¤ì›Œë“œ(ì •ì˜)' í˜•íƒœë¡œ êµì²´
    #     augmented_question = augmented_question.replace(keyword, f"{keyword}({definition})")
    
    # print(f"âœ… ì¦ê°•ëœ í”„ë¡¬í”„íŠ¸:\n{augmented_question}")

    # # --- 4ë‹¨ê³„: Upstage solar-pro2 LLMì„ ì‚¬ìš©í•˜ì—¬ ì˜ì–´ë¡œ ë²ˆì—­ ---
    
    # # solar-pro2 ëª¨ë¸ ì´ˆê¸°í™”
    # llm_pro = ChatUpstage(api_key=llm_api_key, model="solar-1-pro-2-chat")
    
    # # ë²ˆì—­ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜
    # translate_prompt = ChatPromptTemplate.from_template(
    #     "You are a professional translator. Translate the following Korean text into English.\n\nText: {text_to_translate}"
    # )
    
    # # ì¶œë ¥ì„ ë¬¸ìì—´ë¡œ íŒŒì‹±í•˜ëŠ” íŒŒì„œ ì„¤ì •
    # output_parser = StrOutputParser()
    
    # # LCELì„ ì‚¬ìš©í•´ ë²ˆì—­ ì²´ì¸ êµ¬ì„±
    # translate_chain = translate_prompt | llm_pro | output_parser
    
    # print("\n--- 4. ì˜ì–´ë¡œ ë²ˆì—­ ì¤‘... ---")
    # # ì²´ì¸ ì‹¤í–‰
    # final_result = translate_chain.invoke({"text_to_translate": augmented_question})
    # print("âœ… ë²ˆì—­ ì™„ë£Œ!")

    # 1. ì‚¬ìš©í•  ë„êµ¬ ë¦¬ìŠ¤íŠ¸ ì •ì˜
    tools = [get_korean_definition]

    # ë³µì¡í•œ ì¶”ë¡  ë° ë„êµ¬ ì‚¬ìš©ì„ ìœ„í•´ solar-pro2 ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
    llm = ChatUpstage(api_key=llm_api_key, model="solar-pro2")
    # bind_toolsë¥¼ ì‚¬ìš©í•˜ì—¬ LLMì´ get_korean_definition ë„êµ¬ë¥¼ ì¸ì‹í•˜ê³  í˜¸ì¶œí•  ìˆ˜ ìˆë„ë¡ ì„¤ì •
    llm_with_tools = llm.bind_tools(tools)
    # 4. ì „ì²´ ì‘ì—…ì„ ì§€ì‹œí•˜ëŠ” í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±
    prompt = ChatPromptTemplate.from_messages([
        ("system", "You are an expert assistant that augments and translates user prompts following specific steps."),
        ("user", """
        Please perform the following task precisely:
        1. First, identify the key technical terms in the following Korean text. Exclude terms like 'follow-ups' or 'í›„ì† ë…¼ë¬¸'.
        2. For each identified term, you MUST use the 'get_korean_definition' tool to find its definition.
        3. After gathering all definitions, create an augmented Korean text. In this text, insert the definition in parentheses right after each term. For example, if the term is 'ëª¨ë¸' and its definition is '...', it should become 'ëª¨ë¸(...)'.
        4. Finally, translate the **fully augmented Korean text** into English.
        5. Your final output must ONLY be the resulting English translation. Do not include any other explanations or preliminary text.

        Here is the Korean text: "{question}"
        """)
    ])
    # 5. LCELì„ ì‚¬ìš©í•˜ì—¬ ì „ì²´ ì²´ì¸ êµ¬ì„±
    chain = prompt | llm_with_tools | StrOutputParser()

    print("--- Tool Chainì„ ì‚¬ìš©í•˜ì—¬ í”„ë¡¬í”„íŠ¸ ì¦ê°• ë° ë²ˆì—­ ì‹œì‘... ---")
    
    # ì²´ì¸ ì‹¤í–‰
    result = chain.invoke({"question": question})
    
    print("âœ… ì‘ì—… ì™„ë£Œ!")
    return result

if __name__ == "__main__":
    import os 
    from dotenv import load_dotenv
    ROOT_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..', '..', '..')) # 5ë‹¨ê³„ ìœ„ë¡œ ì´ë™
    print(ROOT_DIR)
    load_dotenv(os.path.join(ROOT_DIR, '.env'))
    UPSTAGE_API_KEY = os.getenv("UPSTAGE_API_KEY")
    TAVILY_SEARCH = os.getenv("TAVILY_SEARCH")
    print(UPSTAGE_API_KEY, TAVILY_SEARCH)

    test_question = "Downstream taskì— ëŒ€í•´ ëª¨ë¸ì˜ ì¬ì‚¬ìš©ì„±ì„ í–¥ìƒì‹œí‚¨ í›„ì†ë…¼ë¬¸ì„ ì•Œë ¤ì¤˜."
    print(f"\n--- ì›ë³¸ ì§ˆë¬¸ --- \n{test_question}\n")
    
    # í•¨ìˆ˜ ì‹¤í–‰
    augmented_and_translated_prompt = augment_prompt(
        question=test_question,
        llm_api_key=UPSTAGE_API_KEY,
        tavily_search_key=TAVILY_SEARCH
    )

    print("\n========================================")
    print("    âœ¨ ìµœì¢… ë²ˆì—­ëœ ì¦ê°• í”„ë¡¬í”„íŠ¸ âœ¨")
    print("========================================")
    print(augmented_and_translated_prompt)