from importlib import metadata
from typing import List
import re
from collections import defaultdict
from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_openai import ChatOpenAI
from langchain_upstage import ChatUpstage
from langchain_core.prompts import MessagesPlaceholder

def format_context(context: List[Document]) -> str:
    """
    LangChain Document ë¦¬ìŠ¤íŠ¸ë¥¼ LLM í”„ë¡¬í”„íŠ¸ì— ì í•©í•œ ë‹¨ì¼ ë¬¸ìì—´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    
    :param context: 'title'ì„ metadataì— í¬í•¨í•˜ëŠ” Document ê°ì²´ ë¦¬ìŠ¤íŠ¸
    :return: ê° ë…¼ë¬¸ ì •ë³´ê°€ í¬í•¨ëœ ì „ì²´ ë¬¸ìì—´
    """
    context_parts = []
    for i, doc in enumerate(context):
        # doc.metadataì—ì„œ 'title'ì„, doc.page_contentì—ì„œ ì´ˆë¡ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
        title = doc.metadata.get('title', 'No Title Provided')
        abstract = doc.page_content
        context_parts.append(f"title: {title}\nAbstract: {abstract}\n------------------\n")
    
    return "\n\n".join(context_parts)

def mock_llm_generate(messages, context: List[Document], llm_api_key: str) -> str:
    """
    ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•˜ëŠ” LLM í•¨ìˆ˜.
    ë…¼ë¬¸ë“¤ì„ ë¶„ì„í•˜ì—¬ êµ¬ì¡°í™”ëœ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
    1. Documentì—ëŠ” title(ë…¼ë¬¸ ì œëª©)ê³¼ content(ë…¼ë¬¸ abstract ë‚´ìš©)ê°€ ìˆë‹¤.
    2. contextì˜ ê¸¸ì´ê°€ 0ì´ë©´ "ê²€ìƒ‰ëœ í›„ì†ë…¼ë¬¸ì´ ì—†ë‹¤"ëŠ” ë‚´ìš©ì˜ ë‹µë³€ì„ ë°˜í™˜í•˜ì—¬ë¼. (LLM ì‚¬ìš© ê¸ˆì§€)
    3. format_context í•¨ìˆ˜ë¥¼ í†µí•´ contextì˜ ê° Documentë“¤ì„ `title: ë…¼ë¬¸ ì œëª©, abstract: ë…¼ë¬¸ abstract ë‚´ìš©` ìœ¼ë¡œ ì¬êµ¬ì„±í•˜ì—¬ context_str ì— ì €ì¥í•˜ì—¬ë¼.
    4. promptë¥¼ ì‚¬ìš©í•˜ì—¬ LangChain ë¬¸ë²•ì— ë”°ë¼ LLM(openai GPT-3.5 Turbo)ìœ¼ë¡œë¶€í„° ë‹µë³€ì„ ìƒì„±í•˜ì—¬ë¼.
    
    :param str question: ì‚¬ìš©ìê°€ ì…ë ¥í•œ í”„ë¡¬í”„íŠ¸
    :param List[Document] context: ê²€ìƒ‰ëœ í›„ì† ì—°êµ¬ ë…¼ë¬¸ë“¤ì˜ ë¦¬ìŠ¤íŠ¸
    :param llm_api_key: OpenAI API í‚¤
    :return str: êµ¬ì¡°í™”ëœ ë‹µë³€ ë¬¸ìì—´
    """
    print("ğŸ¤– LLM ë‹µë³€ ìƒì„± ì¤‘...")
    # 2. context ë¦¬ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
    if not context:
        print("â„¹ï¸ ì»¨í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆì–´ LLMì„ í˜¸ì¶œí•˜ì§€ ì•Šê³  ê¸°ë³¸ ë©”ì‹œì§€ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.")
        return "ê²€ìƒ‰ëœ í›„ì† ë…¼ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰í•´ ë³´ì„¸ìš”."
    
    # 3. contextë¥¼ í”„ë¡¬í”„íŠ¸ì— ë„£ê¸° ì¢‹ì€ ë‹¨ì¼ ë¬¸ìì—´ë¡œ formattingí•œë‹¤.
    context_str = format_context(context)

    # 4. LangChain í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì„ ì •ì˜í•©ë‹ˆë‹¤.
    prompt_template = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            """You are a world-class AI research assistant. Your primary goal is to provide a clear, structured, and insightful analysis of academic papers based on their titles and abstracts.
You must synthesize information from multiple sources and present it in an easy-to-digest format for researchers.
Your response must be well-organized, using Markdown for headings and lists.
"""
        ),
        (
            "human",
            """Please read the user's question and the provided follow-up papers.
**If you determine that the provided papers are relevant to the user's question, use them as your primary source for the answer. If not, answer the question based on your own knowledge and the conversation history.**

**User's Question:**
<question>
{question}
</question>

**Provided Context (Follow-up Papers):**
<context>
{context_str}
</context>

**Your Task:**
Directly answer the user's question. Synthesize the key findings from all papers in the context to support your answer. Provide a brief summary in one or two lines for each papers. Focus on what is necessary to answer the question and finally please recommend the best one to read first out of context papers.

**IMPORTANT**: Your final output and all content must be written in **Korean** except for title of the paper.
"""
        ),
    ]
)

    # LLM ëª¨ë¸ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤. (GPT-3.5 Turbo ì‚¬ìš©)
    # llm = ChatOpenAI(model_name="gpt-3.5-turbo", api_key=llm_api_key, temperature=0.2)
    llm = ChatUpstage(model="solar-pro2", api_key=llm_api_key)
    # LangChain Expression Language (LCEL)ì„ ì‚¬ìš©í•˜ì—¬ ì²´ì¸ì„ êµ¬ì„±í•©ë‹ˆë‹¤.
    # 1. í”„ë¡¬í”„íŠ¸ í¬ë§·íŒ… -> 2. LLM í˜¸ì¶œ -> 3. ì¶œë ¥ íŒŒì‹±(ë¬¸ìì—´ë¡œ)
    chain = prompt_template | llm | StrOutputParser()
    
    # ì²´ì¸ì„ ì‹¤í–‰í•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
    answer = chain.invoke({
        "question": messages,
        "context_str": context_str
    })
    print(f"\n\nanswer: {answer}\n\n")
    
    return answer

def mock_llm_generate_no_rag(messages, llm_api_key: str) -> str:
    """
    RAGê°€ í•„ìš”í•˜ì§€ ì•Šì€ ê²½ìš° ì‚¬ìš©í•˜ëŠ” LLM í•¨ìˆ˜.
    LLMì˜ ê¸°ë°˜ì§€ì‹ê³¼ ëŒ€í™” ë‚´ì—­ë“¤ì„ ì´ìš©í•˜ì—¬ ë‹µë³€  
    
    :param str question: ì‚¬ìš©ìê°€ ì…ë ¥í•œ í”„ë¡¬í”„íŠ¸
    :param llm_api_key: OpenAI API í‚¤
    :return str: ë‹µë³€ ë¬¸ìì—´
    """
    print("ğŸ¤– LLM ë‹µë³€ ìƒì„± ì¤‘...")
    
    # 4. LangChain í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì„ ì •ì˜í•©ë‹ˆë‹¤.
    prompt_template = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            """You are a world-class AI research assistant. Your primary goal is to provide a clear, structured, and insightful analysis of academic papers based on their titles and abstracts.
You must synthesize information from multiple sources and present it in an easy-to-digest format for researchers.
Your response must be well-organized, using Markdown for headings and lists.
"""
        ),
        (
            "human",
            """
**User's Question:**
<question>
{question}
</question>

**Your Task:**
Directly answer the user's question.

**IMPORTANT**: Your final output and all content must be written in **Korean** except for title of the paper.
"""
        ),
    ]
)

    # LLM ëª¨ë¸ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤. (GPT-3.5 Turbo ì‚¬ìš©)
    # llm = ChatOpenAI(model_name="gpt-3.5-turbo", api_key=llm_api_key, temperature=0.2)
    llm = ChatUpstage(model="solar-pro2", api_key=llm_api_key)
    # LangChain Expression Language (LCEL)ì„ ì‚¬ìš©í•˜ì—¬ ì²´ì¸ì„ êµ¬ì„±í•©ë‹ˆë‹¤.
    # 1. í”„ë¡¬í”„íŠ¸ í¬ë§·íŒ… -> 2. LLM í˜¸ì¶œ -> 3. ì¶œë ¥ íŒŒì‹±(ë¬¸ìì—´ë¡œ)
    chain = prompt_template | llm | StrOutputParser()
    
    # ì²´ì¸ì„ ì‹¤í–‰í•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
    answer = chain.invoke({
        "question": messages
    })
    print(f"\n\nanswer: {answer}\n\n")
    
    return answer


def rag_judge(question: str, llm_api_key: str) -> str:
    """
    ì‚¬ìš©ìì˜ ì¿¼ë¦¬ë¥¼ ë¶„ì„í•˜ì—¬ RAGê°€ í•„ìš”í•œì§€ íŒë‹¨
    """

    prompt_template = ChatPromptTemplate.from_messages(
    [
        (
                "system",
                """You are an AI designed to determine whether a user's question requires external knowledge retrieval (RAG).

Your judgment must be based on the following criteria:
- 'RAG': The question asks for information on specific papers, data, recent research, technical terms, or facts that are not common knowledge.
- 'NO_RAG': The question is about general knowledge, greetings, personal feelings, or a summary of previous conversation content.

Your response must be exclusively 'RAG' or 'NO_RAG'. Do not include any additional explanations or text."""
            ),
            (
              "human",
              """Question: {question}

              Please judge whether the question requires external knowledge retrieval (RAG).
              """
            )
    ])

    llm = ChatUpstage(model="solar-pro2", api_key=llm_api_key)
    chain = prompt_template | llm | StrOutputParser()

    judgement = chain.invoke({
        "question": question
    })

    return judgement
