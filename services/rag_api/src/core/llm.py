from importlib import metadata
from typing import List
import re
from collections import defaultdict
from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_openai import ChatOpenAI

def format_context(context: List[Document]) -> str:
    """
    LangChain Document ë¦¬ìŠ¤íŠ¸ë¥¼ LLM í”„ë¡¬í”„íŠ¸ì— ì í•©í•œ ë‹¨ì¼ ë¬¸ìì—´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    
    :param context: 'title'ì„ metadataì— í¬í•¨í•˜ëŠ” Document ê°ì²´ ë¦¬ìŠ¤íŠ¸
    :return: ê° ë…¼ë¬¸ ì •ë³´ê°€ í¬í•¨ëœ ì „ì²´ ë¬¸ìì—´
    """
    context_parts = []
    for i, doc in enumerate(context):
        # doc.metadataì—ì„œ 'title'ì„, doc.page_contentì—ì„œ ì´ˆë¡ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
        title = doc.metadata.get('title', 'No Title Provided')
        abstract = doc.page_content
        context_parts.append(f"--- Paper {i+1} ---\nTitle: {title}\nAbstract: {abstract}")
    
    return "\n\n".join(context_parts)

def mock_llm_generate(question: str, context: List[Document], llm_api_key: str) -> str:
    """
    ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•˜ëŠ” LLM í•¨ìˆ˜.
    ë…¼ë¬¸ë“¤ì„ ë¶„ì„í•˜ì—¬ êµ¬ì¡°í™”ëœ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
    1. Documentì—ëŠ” title(ë…¼ë¬¸ ì œëª©)ê³¼ content(ë…¼ë¬¸ abstract ë‚´ìš©)ê°€ ìˆë‹¤.
    2. contextì˜ ê¸¸ì´ê°€ 0ì´ë©´ "ê²€ìƒ‰ëœ í›„ì†ë…¼ë¬¸ì´ ì—†ë‹¤"ëŠ” ë‚´ìš©ì˜ ë‹µë³€ì„ ë°˜í™˜í•˜ì—¬ë¼. (LLM ì‚¬ìš© ê¸ˆì§€)
    3. format_context í•¨ìˆ˜ë¥¼ í†µí•´ contextì˜ ê° Documentë“¤ì„ `title: ë…¼ë¬¸ ì œëª©, abstract: ë…¼ë¬¸ abstract ë‚´ìš©` ìœ¼ë¡œ ì¬êµ¬ì„±í•˜ì—¬ context_str ì— ì €ì¥í•˜ì—¬ë¼.
    4. promptë¥¼ ì‚¬ìš©í•˜ì—¬ LangChain ë¬¸ë²•ì— ë”°ë¼ LLM(openai GPT-3.5 Turbo)ìœ¼ë¡œë¶€í„° ë‹µë³€ì„ ìƒì„±í•˜ì—¬ë¼.
    
    :param str question: ì‚¬ìš©ìê°€ ì…ë ¥í•œ í”„ë¡¬í”„íŠ¸
    :param List[Document] context: ê²€ìƒ‰ëœ í›„ì† ì—°êµ¬ ë…¼ë¬¸ë“¤ì˜ ë¦¬ìŠ¤íŠ¸
    :param llm_api_key: OpenAI API í‚¤
    :return str: êµ¬ì¡°í™”ëœ ë‹µë³€ ë¬¸ìì—´
    """
    print("ğŸ¤– LLM ë‹µë³€ ìƒì„± ì¤‘...")
    # 2. context ë¦¬ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
    if not context:
        print("â„¹ï¸ ì»¨í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆì–´ LLMì„ í˜¸ì¶œí•˜ì§€ ì•Šê³  ê¸°ë³¸ ë©”ì‹œì§€ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.")
        return "ê²€ìƒ‰ëœ í›„ì† ë…¼ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰í•´ ë³´ì„¸ìš”."
    
    # 3. contextë¥¼ í”„ë¡¬í”„íŠ¸ì— ë„£ê¸° ì¢‹ì€ ë‹¨ì¼ ë¬¸ìì—´ë¡œ formattingí•œë‹¤.
    context_str = format_context(context)

    # 4. LangChain í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì„ ì •ì˜í•©ë‹ˆë‹¤.
    prompt_template = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                """You are a world-class AI research assistant. Your primary goal is to provide a clear, structured, and insightful analysis of academic papers based on their titles and abstracts.
You must synthesize information from multiple sources and present it in an easy-to-digest format for researchers.
Your response must be well-organized, using Markdown for headings and lists.
"""
            ),
            (
                "human",
                """Based on the following research papers, please answer my question.

**User's Question:**
<question>
{question}
</question>

**Provided Context (Follow-up Papers):**
<context>
{context_str}
</context>

**Your Task:**
Directly answer the user's question. Synthesize the key findings from all papers in the context to support your answer. Provide a brief summary in one or two lines for each papers. Focus on what is necessary to answer the question and finally please recommend the best one to read first out of context papers.

**IMPORTANT**: Your final output and all content must be written in **Korean**.
"""
            ),
        ]
    )

    # LLM ëª¨ë¸ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤. (GPT-3.5 Turbo ì‚¬ìš©)
    llm = ChatOpenAI(model_name="gpt-3.5-turbo", api_key=llm_api_key, temperature=0.2)
    
    # LangChain Expression Language (LCEL)ì„ ì‚¬ìš©í•˜ì—¬ ì²´ì¸ì„ êµ¬ì„±í•©ë‹ˆë‹¤.
    # 1. í”„ë¡¬í”„íŠ¸ í¬ë§·íŒ… -> 2. LLM í˜¸ì¶œ -> 3. ì¶œë ¥ íŒŒì‹±(ë¬¸ìì—´ë¡œ)
    chain = prompt_template | llm | StrOutputParser()
    
    # ì²´ì¸ì„ ì‹¤í–‰í•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
    answer = chain.invoke({
        "question": question,
        "context_str": context_str
    })
    
    return answer

def analyze_papers(papers: List[str]) -> dict:
    """
    ë…¼ë¬¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ì—¬ ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë¶„ë¥˜í•˜ê³  ì¤‘ìš”ë„ ë¶„ì„
    
    :param papers: ë…¼ë¬¸ ë¦¬ìŠ¤íŠ¸
    :return: ë¶„ì„ëœ ë…¼ë¬¸ ì •ë³´
    """
    analysis = {
        'total_count': len(papers),
        'categories': defaultdict(list),
        'key_papers': [],
        'research_trends': []
    }
    
    for paper in papers:
        # ë…¼ë¬¸ ì œëª©ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
        keywords = extract_keywords(paper)
        
        # ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
        category = classify_paper_category(paper, keywords)
        analysis['categories'][category].append(paper)
        
        # ì¤‘ìš”ë„ í‰ê°€ (ì œëª© ê¸¸ì´, íŠ¹ì • í‚¤ì›Œë“œ í¬í•¨ ì—¬ë¶€ ë“±)
        importance = evaluate_importance(paper, keywords)
        if importance > 0.7:  # ì¤‘ìš”ë„ê°€ ë†’ì€ ë…¼ë¬¸
            analysis['key_papers'].append((paper, importance))
    
    # ì—°êµ¬ íŠ¸ë Œë“œ ë¶„ì„
    analysis['research_trends'] = analyze_research_trends(analysis['categories'])
    
    return analysis

def extract_keywords(paper: str) -> List[str]:
    """ë…¼ë¬¸ ì œëª©ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
    # ì¼ë°˜ì ì¸ AI/ML í‚¤ì›Œë“œë“¤
    ai_keywords = [
        'neural', 'network', 'learning', 'deep', 'transformer', 'attention',
        'bert', 'gpt', 'llm', 'rag', 'retrieval', 'generation', 'embedding',
        'vector', 'graph', 'nlp', 'computer vision', 'reinforcement',
        'optimization', 'architecture', 'model', 'algorithm'
    ]
    
    paper_lower = paper.lower()
    found_keywords = []
    
    for keyword in ai_keywords:
        if keyword in paper_lower:
            found_keywords.append(keyword)
    
    return found_keywords

def classify_paper_category(paper: str, keywords: List[str]) -> str:
    """ë…¼ë¬¸ì„ ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë¶„ë¥˜"""
    paper_lower = paper.lower()
    
    if any(word in paper_lower for word in ['transformer', 'attention', 'bert', 'gpt', 'llm']):
        return 'Language Models'
    elif any(word in paper_lower for word in ['rag', 'retrieval', 'generation']):
        return 'Retrieval-Augmented Generation'
    elif any(word in paper_lower for word in ['neural', 'network', 'deep']):
        return 'Neural Networks'
    elif any(word in paper_lower for word in ['graph', 'gnn']):
        return 'Graph Neural Networks'
    elif any(word in paper_lower for word in ['computer vision', 'image', 'vision']):
        return 'Computer Vision'
    elif any(word in paper_lower for word in ['reinforcement', 'rl']):
        return 'Reinforcement Learning'
    else:
        return 'General AI/ML'

def evaluate_importance(paper: str, keywords: List[str]) -> float:
    """ë…¼ë¬¸ì˜ ì¤‘ìš”ë„ í‰ê°€ (0.0 ~ 1.0)"""
    importance = 0.0
    
    # í‚¤ì›Œë“œ ìˆ˜ì— ë”°ë¥¸ ì ìˆ˜
    importance += min(len(keywords) * 0.1, 0.3)
    
    # ì œëª© ê¸¸ì´ì— ë”°ë¥¸ ì ìˆ˜ (ì ë‹¹í•œ ê¸¸ì´ê°€ ì¢‹ìŒ)
    title_length = len(paper)
    if 20 <= title_length <= 80:
        importance += 0.2
    elif title_length > 80:
        importance += 0.1
    
    # íŠ¹ì • ì¤‘ìš” í‚¤ì›Œë“œì— ëŒ€í•œ ë³´ë„ˆìŠ¤
    if any(word in paper.lower() for word in ['survey', 'review', 'comprehensive']):
        importance += 0.3
    
    if any(word in paper.lower() for word in ['state-of-the-art', 'sota', 'breakthrough']):
        importance += 0.2
    
    return min(importance, 1.0)

def analyze_research_trends(categories: dict) -> List[str]:
    """ì—°êµ¬ íŠ¸ë Œë“œ ë¶„ì„"""
    trends = []
    
    # ê°€ì¥ ë§ì€ ë…¼ë¬¸ì´ ìˆëŠ” ì¹´í…Œê³ ë¦¬
    if categories:
        most_popular = max(categories.items(), key=lambda x: len(x[1]))
        trends.append(f"'{most_popular[0]}' ë¶„ì•¼ê°€ ê°€ì¥ í™œë°œí•˜ê²Œ ì—°êµ¬ë˜ê³  ìˆìŠµë‹ˆë‹¤.")
    
    # ë‹¤ì–‘í•œ ë¶„ì•¼ê°€ ì—°êµ¬ë˜ê³  ìˆëŠ”ì§€
    if len(categories) >= 3:
        trends.append("ë‹¤ì–‘í•œ AI ë¶„ì•¼ì—ì„œ í›„ì† ì—°êµ¬ê°€ ì§„í–‰ë˜ê³  ìˆìŠµë‹ˆë‹¤.")
    elif len(categories) == 2:
        trends.append("ì£¼ë¡œ ë‘ ë¶„ì•¼ì—ì„œ í›„ì† ì—°êµ¬ê°€ ì§‘ì¤‘ë˜ê³  ìˆìŠµë‹ˆë‹¤.")
    else:
        trends.append("íŠ¹ì • ë¶„ì•¼ì— ì§‘ì¤‘ëœ í›„ì† ì—°êµ¬ê°€ ì§„í–‰ë˜ê³  ìˆìŠµë‹ˆë‹¤.")
    
    return trends

def generate_structured_answer(analysis: dict) -> str:
    """êµ¬ì¡°í™”ëœ ë‹µë³€ ìƒì„±"""
    answer_parts = []
    
    # í—¤ë”
    answer_parts.append("ğŸ” **í›„ì† ì—°êµ¬ ë¶„ì„ ê²°ê³¼**")
    answer_parts.append(f"ì´ {analysis['total_count']}ê°œì˜ ê´€ë ¨ ë…¼ë¬¸ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.\n")
    
    # ì—°êµ¬ íŠ¸ë Œë“œ
    if analysis['research_trends']:
        answer_parts.append("ğŸ“ˆ **ì—°êµ¬ íŠ¸ë Œë“œ**")
        for trend in analysis['research_trends']:
            answer_parts.append(f"â€¢ {trend}")
        answer_parts.append("")
    
    # ì¹´í…Œê³ ë¦¬ë³„ ë¶„ë¥˜
    if analysis['categories']:
        answer_parts.append("ğŸ“š **ë¶„ì•¼ë³„ ë¶„ë¥˜**")
        for category, papers in analysis['categories'].items():
            answer_parts.append(f"â€¢ **{category}**: {len(papers)}ê°œ ë…¼ë¬¸")
        answer_parts.append("")
    
    # ì£¼ìš” ë…¼ë¬¸ë“¤
    if analysis['key_papers']:
        answer_parts.append("â­ **ì£¼ìš” ë…¼ë¬¸**")
        # ì¤‘ìš”ë„ ìˆœìœ¼ë¡œ ì •ë ¬
        sorted_papers = sorted(analysis['key_papers'], key=lambda x: x[1], reverse=True)
        for paper, importance in sorted_papers[:5]:  # ìƒìœ„ 5ê°œë§Œ
            importance_stars = "â­" * int(importance * 5)
            answer_parts.append(f"â€¢ {importance_stars} {paper}")
        answer_parts.append("")
    
    # ìƒì„¸ ë…¼ë¬¸ ëª©ë¡
    answer_parts.append("ğŸ“– **ì „ì²´ ë…¼ë¬¸ ëª©ë¡**")
    for i, paper in enumerate(analysis['categories'].values(), 1):
        for p in paper:
            answer_parts.append(f"{i}. {p}")
            i += 1
    
    return "\n".join(answer_parts)


if __name__ == "__main__":
    from dotenv import load_dotenv
    import os
    ROOT_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..', '..', '..')) # 5ë‹¨ê³„ ìœ„ë¡œ ì´ë™
    print(ROOT_DIR)
    load_dotenv(os.path.join(ROOT_DIR, '.env'))
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
    print(OPENAI_API_KEY)
    p1 = Document(metadata={'title':'Attention is all you need'}, page_content="The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.")
    p2 = Document(metadata={'title':'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding'}, page_content='We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).')
    p3 = Document(metadata={'title':'BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension'}, page_content='We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of the original sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD, achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 6 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also report ablation experiments that replicate other pretraining schemes within the BART framework, to better measure which factors most influence end-task performance.')
    p4 = Document(metadata={'title':'Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer'}, page_content='Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.')
    p5 = Document(metadata={'title':'Deep contextualized word representations'}, page_content='We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.')
    context = [p1,p2,p3,p4,p5]

    question = "RNN ì´í›„ì— LLM ë¶„ì•¼ì˜ ì‹œì‘ì„ ì•Œë¦° ë…¼ë¬¸ë“¤ì„ ì¶”ì²œí•´ì¤˜."

    answer = mock_llm_generate(question, context, OPENAI_API_KEY)

    print(answer)