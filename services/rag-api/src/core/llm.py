from typing import List
import re
from collections import defaultdict

def mock_llm_generate(context: List[str]) -> str:
    """
    ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•˜ëŠ” LLM ëª¨ì˜ í•¨ìˆ˜.
    ë…¼ë¬¸ë“¤ì„ ë¶„ì„í•˜ì—¬ êµ¬ì¡°í™”ëœ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
    
    :param context: ê²€ìƒ‰ëœ í›„ì† ì—°êµ¬ ë…¼ë¬¸ë“¤ì˜ ë¦¬ìŠ¤íŠ¸
    :return: êµ¬ì¡°í™”ëœ ë‹µë³€ ë¬¸ìì—´
    """
    print("ğŸ¤– LLM ë‹µë³€ ìƒì„± ì¤‘...")
    
    if not context:
        return "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ëœ í›„ì† ì—°êµ¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    # ë…¼ë¬¸ë“¤ì„ ë¶„ì„í•˜ì—¬ êµ¬ì¡°í™”
    analyzed_papers = analyze_papers(context)
    
    # êµ¬ì¡°í™”ëœ ë‹µë³€ ìƒì„±
    answer = generate_structured_answer(analyzed_papers)
    
    return answer

def analyze_papers(papers: List[str]) -> dict:
    """
    ë…¼ë¬¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ì—¬ ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë¶„ë¥˜í•˜ê³  ì¤‘ìš”ë„ ë¶„ì„
    
    :param papers: ë…¼ë¬¸ ë¦¬ìŠ¤íŠ¸
    :return: ë¶„ì„ëœ ë…¼ë¬¸ ì •ë³´
    """
    analysis = {
        'total_count': len(papers),
        'categories': defaultdict(list),
        'key_papers': [],
        'research_trends': []
    }
    
    for paper in papers:
        # ë…¼ë¬¸ ì œëª©ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
        keywords = extract_keywords(paper)
        
        # ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
        category = classify_paper_category(paper, keywords)
        analysis['categories'][category].append(paper)
        
        # ì¤‘ìš”ë„ í‰ê°€ (ì œëª© ê¸¸ì´, íŠ¹ì • í‚¤ì›Œë“œ í¬í•¨ ì—¬ë¶€ ë“±)
        importance = evaluate_importance(paper, keywords)
        if importance > 0.7:  # ì¤‘ìš”ë„ê°€ ë†’ì€ ë…¼ë¬¸
            analysis['key_papers'].append((paper, importance))
    
    # ì—°êµ¬ íŠ¸ë Œë“œ ë¶„ì„
    analysis['research_trends'] = analyze_research_trends(analysis['categories'])
    
    return analysis

def extract_keywords(paper: str) -> List[str]:
    """ë…¼ë¬¸ ì œëª©ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
    # ì¼ë°˜ì ì¸ AI/ML í‚¤ì›Œë“œë“¤
    ai_keywords = [
        'neural', 'network', 'learning', 'deep', 'transformer', 'attention',
        'bert', 'gpt', 'llm', 'rag', 'retrieval', 'generation', 'embedding',
        'vector', 'graph', 'nlp', 'computer vision', 'reinforcement',
        'optimization', 'architecture', 'model', 'algorithm'
    ]
    
    paper_lower = paper.lower()
    found_keywords = []
    
    for keyword in ai_keywords:
        if keyword in paper_lower:
            found_keywords.append(keyword)
    
    return found_keywords

def classify_paper_category(paper: str, keywords: List[str]) -> str:
    """ë…¼ë¬¸ì„ ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë¶„ë¥˜"""
    paper_lower = paper.lower()
    
    if any(word in paper_lower for word in ['transformer', 'attention', 'bert', 'gpt', 'llm']):
        return 'Language Models'
    elif any(word in paper_lower for word in ['rag', 'retrieval', 'generation']):
        return 'Retrieval-Augmented Generation'
    elif any(word in paper_lower for word in ['neural', 'network', 'deep']):
        return 'Neural Networks'
    elif any(word in paper_lower for word in ['graph', 'gnn']):
        return 'Graph Neural Networks'
    elif any(word in paper_lower for word in ['computer vision', 'image', 'vision']):
        return 'Computer Vision'
    elif any(word in paper_lower for word in ['reinforcement', 'rl']):
        return 'Reinforcement Learning'
    else:
        return 'General AI/ML'

def evaluate_importance(paper: str, keywords: List[str]) -> float:
    """ë…¼ë¬¸ì˜ ì¤‘ìš”ë„ í‰ê°€ (0.0 ~ 1.0)"""
    importance = 0.0
    
    # í‚¤ì›Œë“œ ìˆ˜ì— ë”°ë¥¸ ì ìˆ˜
    importance += min(len(keywords) * 0.1, 0.3)
    
    # ì œëª© ê¸¸ì´ì— ë”°ë¥¸ ì ìˆ˜ (ì ë‹¹í•œ ê¸¸ì´ê°€ ì¢‹ìŒ)
    title_length = len(paper)
    if 20 <= title_length <= 80:
        importance += 0.2
    elif title_length > 80:
        importance += 0.1
    
    # íŠ¹ì • ì¤‘ìš” í‚¤ì›Œë“œì— ëŒ€í•œ ë³´ë„ˆìŠ¤
    if any(word in paper.lower() for word in ['survey', 'review', 'comprehensive']):
        importance += 0.3
    
    if any(word in paper.lower() for word in ['state-of-the-art', 'sota', 'breakthrough']):
        importance += 0.2
    
    return min(importance, 1.0)

def analyze_research_trends(categories: dict) -> List[str]:
    """ì—°êµ¬ íŠ¸ë Œë“œ ë¶„ì„"""
    trends = []
    
    # ê°€ì¥ ë§ì€ ë…¼ë¬¸ì´ ìˆëŠ” ì¹´í…Œê³ ë¦¬
    if categories:
        most_popular = max(categories.items(), key=lambda x: len(x[1]))
        trends.append(f"'{most_popular[0]}' ë¶„ì•¼ê°€ ê°€ì¥ í™œë°œí•˜ê²Œ ì—°êµ¬ë˜ê³  ìˆìŠµë‹ˆë‹¤.")
    
    # ë‹¤ì–‘í•œ ë¶„ì•¼ê°€ ì—°êµ¬ë˜ê³  ìˆëŠ”ì§€
    if len(categories) >= 3:
        trends.append("ë‹¤ì–‘í•œ AI ë¶„ì•¼ì—ì„œ í›„ì† ì—°êµ¬ê°€ ì§„í–‰ë˜ê³  ìˆìŠµë‹ˆë‹¤.")
    elif len(categories) == 2:
        trends.append("ì£¼ë¡œ ë‘ ë¶„ì•¼ì—ì„œ í›„ì† ì—°êµ¬ê°€ ì§‘ì¤‘ë˜ê³  ìˆìŠµë‹ˆë‹¤.")
    else:
        trends.append("íŠ¹ì • ë¶„ì•¼ì— ì§‘ì¤‘ëœ í›„ì† ì—°êµ¬ê°€ ì§„í–‰ë˜ê³  ìˆìŠµë‹ˆë‹¤.")
    
    return trends

def generate_structured_answer(analysis: dict) -> str:
    """êµ¬ì¡°í™”ëœ ë‹µë³€ ìƒì„±"""
    answer_parts = []
    
    # í—¤ë”
    answer_parts.append("ğŸ” **í›„ì† ì—°êµ¬ ë¶„ì„ ê²°ê³¼**")
    answer_parts.append(f"ì´ {analysis['total_count']}ê°œì˜ ê´€ë ¨ ë…¼ë¬¸ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.\n")
    
    # ì—°êµ¬ íŠ¸ë Œë“œ
    if analysis['research_trends']:
        answer_parts.append("ğŸ“ˆ **ì—°êµ¬ íŠ¸ë Œë“œ**")
        for trend in analysis['research_trends']:
            answer_parts.append(f"â€¢ {trend}")
        answer_parts.append("")
    
    # ì¹´í…Œê³ ë¦¬ë³„ ë¶„ë¥˜
    if analysis['categories']:
        answer_parts.append("ğŸ“š **ë¶„ì•¼ë³„ ë¶„ë¥˜**")
        for category, papers in analysis['categories'].items():
            answer_parts.append(f"â€¢ **{category}**: {len(papers)}ê°œ ë…¼ë¬¸")
        answer_parts.append("")
    
    # ì£¼ìš” ë…¼ë¬¸ë“¤
    if analysis['key_papers']:
        answer_parts.append("â­ **ì£¼ìš” ë…¼ë¬¸**")
        # ì¤‘ìš”ë„ ìˆœìœ¼ë¡œ ì •ë ¬
        sorted_papers = sorted(analysis['key_papers'], key=lambda x: x[1], reverse=True)
        for paper, importance in sorted_papers[:5]:  # ìƒìœ„ 5ê°œë§Œ
            importance_stars = "â­" * int(importance * 5)
            answer_parts.append(f"â€¢ {importance_stars} {paper}")
        answer_parts.append("")
    
    # ìƒì„¸ ë…¼ë¬¸ ëª©ë¡
    answer_parts.append("ğŸ“– **ì „ì²´ ë…¼ë¬¸ ëª©ë¡**")
    for i, paper in enumerate(analysis['categories'].values(), 1):
        for p in paper:
            answer_parts.append(f"{i}. {p}")
            i += 1
    
    return "\n".join(answer_parts)
